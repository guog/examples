## 使用本地运行的LLM

本想直接使用OpenAI，放弃了，也折腾了几天，原因：
1、购买麻烦，只允许国外信用卡，可以使用代理公司，比较费劲；
2、需要科学上网，但容易被封；
3、能力受限，不能为所欲为；

最终决定本地跑LLM，已经在本地运行 llama3.2:3b ,相当于本地无限 Chatgpt4。

![03-llama](./assets/03-llama.jpg)



## 几篇值得一读的文章

1 、使用OpenAI生成图表(Chart) https://medium.com/@sumitsahoo/generate-charts-using-openai-code-interpreter-88cb93a06da0

2、使用OpenAI输出结构化数据 https://openai.com/index/introducing-structured-outputs-in-the-api/

3、介绍RAG：让LLM检索私有数据，而不必训练LLM，最便宜可行的方式。https://blog.myli.page/why-rag-is-big-aa60282693dc 

> RAG overcomes all three weaknesses of the fine-tuning approach:
>
> - There’s no training involved, so it’s **cheap**.
> - Data is fetched only when you ask for them, so it’s **always up to date**.
> - The framework can show you the retrieved documents, so it’s **more trustworthy**.

*对公司来说，训练私有LLM的成本太高了。*

## 后续

继续研究 [LlamaIndex](https://docs.llamaindex.ai/) 框架，这应该是我们想要的玩法，可以将我们的私有数据与多种LLMs相结合。