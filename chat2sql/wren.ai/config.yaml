# you should rename this file to config.yaml and put it in ~/.wrenai
# please pay attention to the comments starting with # and adjust the config accordingly

type: llm
provider: litellm_llm
timeout: 3000
models:
- api_base: http://host.docker.internal:11434/v1  # change this to your ollama host, api_base should be <ollama_url>/v1
  model: openai/deepseek-r1:14b  # openai/<ollama_model_name>
  api_key_name: LLM_LM_STUDIO_API_KEY
  kwargs:
    n: 1
    temperature: 0

---
type: embedder
provider: ollama_embedder
models:
  - model: bge-large  # put your ollama embedder model name here
#    dimension: 1024
url: http://host.docker.internal:11434  # change this to your ollama host, url should be <ollama_url>
timeout: 3000

---
type: engine
provider: wren_ui
endpoint: http://wren-ui:3000

---
type: document_store
provider: qdrant
location: http://qdrant:6333
embedding_model_dim: 1024  # put your embedding model dimension here
timeout: 3000
recreate_index: false
recreate_collection: true

---
# the format of llm and embedder should be <provider>.<model_name> such as litellm_llm.gpt-4o-2024-08-06
# the pipes may be not the latest version, please refer to the latest version: https://raw.githubusercontent.com/canner/WrenAI/<WRENAI_VERSION_NUMBER>/docker/config.example.yaml
type: pipeline
pipes:
  - name: db_schema_indexing
    embedder: ollama_embedder.bge-large
    document_store: qdrant
  - name: historical_question_indexing
    embedder: ollama_embedder.bge-large
    document_store: qdrant
  - name: table_description_indexing
    embedder: ollama_embedder.bge-large
    document_store: qdrant
  - name: db_schema_retrieval
    llm: litellm_llm.openai/deepseek-r1:14b
    embedder: ollama_embedder.bge-large
    document_store: qdrant
  - name: historical_question_retrieval
    embedder: ollama_embedder.bge-large
    document_store: qdrant
  - name: sql_generation
    llm: litellm_llm.openai/deepseek-r1:14b
    engine: wren_ui
  - name: sql_correction
    llm: litellm_llm.openai/deepseek-r1:14b
    engine: wren_ui
  - name: followup_sql_generation
    llm: litellm_llm.openai/deepseek-r1:14b
    engine: wren_ui
  - name: sql_summary
    llm: litellm_llm.openai/deepseek-r1:14b
  - name: sql_answer
    llm: litellm_llm.openai/deepseek-r1:14b
    engine: wren_ui
  - name: sql_breakdown
    llm: litellm_llm.openai/deepseek-r1:14b
    engine: wren_ui
  - name: sql_expansion
    llm: litellm_llm.openai/deepseek-r1:14b
    engine: wren_ui
  - name: sql_explanation
    llm: litellm_llm.openai/deepseek-r1:14b
  - name: sql_regeneration
    llm: litellm_llm.openai/deepseek-r1:14b
    engine: wren_ui
  - name: semantics_description
    llm: litellm_llm.openai/deepseek-r1:14b
  - name: relationship_recommendation
    llm: litellm_llm.openai/deepseek-r1:14b
    engine: wren_ui
  - name: question_recommendation
    llm: litellm_llm.openai/deepseek-r1:14b
  - name: question_recommendation_db_schema_retrieval
    llm: litellm_llm.openai/deepseek-r1:14b
    embedder: ollama_embedder.bge-large
    document_store: qdrant
  - name: question_recommendation_sql_generation
    llm: litellm_llm.openai/deepseek-r1:14b
    engine: wren_ui
  - name: chart_generation
    llm: litellm_llm.openai/deepseek-r1:14b
  - name: chart_adjustment
    llm: litellm_llm.openai/deepseek-r1:14b
  - name: intent_classification
    llm: litellm_llm.openai/deepseek-r1:14b
    embedder: ollama_embedder.bge-large
    document_store: qdrant
  - name: data_assistance
    llm: litellm_llm.openai/deepseek-r1:14b
  - name: sql_pairs_indexing
    document_store: qdrant
    embedder: ollama_embedder.bge-large
  - name: sql_pairs_deletion
    document_store: qdrant
    embedder: ollama_embedder.bge-large 
  - name: sql_pairs_retrieval
    document_store: qdrant
    embedder: ollama_embedder.bge-large
    llm: litellm_llm.openai/deepseek-r1:14b
  - name: preprocess_sql_data
    llm: litellm_llm.openai/deepseek-r1:14b
  - name: sql_executor
    engine: wren_ui
  - name: sql_question_generation
    llm: litellm_llm.openai/deepseek-r1:14b
---
settings:
  column_indexing_batch_size: 500
  table_retrieval_size: 100
  table_column_retrieval_size: 1000
  allow_using_db_schemas_without_pruning: false
  query_cache_maxsize: 1000
  query_cache_ttl: 36000
  langfuse_host: https://cloud.langfuse.com
  langfuse_enable: true
  logging_level: DEBUG
  development: true